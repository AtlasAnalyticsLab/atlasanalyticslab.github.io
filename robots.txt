---
layout: null
permalink: /robots.txt
---
# robots.txt - Crawler Access Control
#
# Purpose:
#   Controls which web crawlers can access the site and how frequently.
#   Helps prevent excessive crawler traffic and bandwidth usage.
#
# Note:
#   - Good crawlers (Google, Bing) respect these rules
#   - Malicious crawlers may ignore this file
#   - For GitHub Pages, this provides basic protection

# Allow major search engines.
# Note: Googlebot ignores Crawl-delay directives, so we omit it to avoid Search Console warnings.
User-agent: Googlebot
Disallow: /images/
Disallow: /assets/
Disallow: /_site/
Disallow: /bin/
Disallow: /CNAME
Disallow: /README.md
Disallow: /DEVELOPMENT.md
Disallow: /.htaccess

User-agent: Bingbot
Crawl-delay: 10
Disallow: /images/
Disallow: /assets/
Disallow: /_site/
Disallow: /bin/
Disallow: /CNAME
Disallow: /README.md
Disallow: /DEVELOPMENT.md
Disallow: /.htaccess

User-agent: Slurp
Crawl-delay: 10
Disallow: /images/
Disallow: /assets/
Disallow: /_site/
Disallow: /bin/
Disallow: /CNAME
Disallow: /README.md
Disallow: /DEVELOPMENT.md
Disallow: /.htaccess

# Block aggressive/problematic crawlers
User-agent: MJ12bot
Disallow: /

User-agent: AhrefsBot
Disallow: /

User-agent: SemrushBot
Disallow: /

User-agent: DotBot
Disallow: /

User-agent: PetalBot
Disallow: /

User-agent: DataForSeoBot
Disallow: /

# Block AI training crawlers (optional - uncomment if you want to block)
# User-agent: GPTBot
# Disallow: /
#
# User-agent: CCBot
# Disallow: /
#
# User-agent: anthropic-ai
# Disallow: /
#
# User-agent: Claude-Web
# Disallow: /

# Default rule for unlisted crawlers
User-agent: *
Crawl-delay: 10
Disallow: /images/
Disallow: /assets/
Disallow: /_site/
Disallow: /bin/
Disallow: /CNAME
Disallow: /README.md
Disallow: /DEVELOPMENT.md
Disallow: /.htaccess

# Allow access to main pages (everything else is allowed by default)
Allow: /

# Sitemap location (helps good crawlers index efficiently)
Sitemap: {{ site.url }}{{ site.baseurl }}/sitemap.xml
