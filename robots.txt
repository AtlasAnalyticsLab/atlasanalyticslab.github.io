---
layout: null
permalink: /robots.txt
---
# robots.txt - Crawler Access Control
#
# Purpose:
#   Controls which web crawlers can access the site and how frequently.
#   Helps prevent excessive crawler traffic and bandwidth usage.
#
# Note:
#   - Good crawlers (Google, Bing) respect these rules
#   - Malicious crawlers may ignore this file
#   - For GitHub Pages, this provides basic protection

# Allow major search engines with rate limiting
User-agent: Googlebot
Crawl-delay: 10
Allow: /

User-agent: Bingbot
Crawl-delay: 10
Allow: /

User-agent: Slurp
Crawl-delay: 10
Allow: /

# Block aggressive/problematic crawlers
User-agent: MJ12bot
Disallow: /

User-agent: AhrefsBot
Disallow: /

User-agent: SemrushBot
Disallow: /

User-agent: DotBot
Disallow: /

User-agent: PetalBot
Disallow: /

User-agent: DataForSeoBot
Disallow: /

# Block AI training crawlers (optional - uncomment if you want to block)
# User-agent: GPTBot
# Disallow: /
#
# User-agent: CCBot
# Disallow: /
#
# User-agent: anthropic-ai
# Disallow: /
#
# User-agent: Claude-Web
# Disallow: /

# Default rule for unlisted crawlers
User-agent: *
Crawl-delay: 10
Disallow: /images/
Disallow: /assets/
Disallow: /_site/

# Allow access to main pages
Allow: /$
Allow: /allnews
Allow: /allnews.html
Allow: /team
Allow: /publications
Allow: /contact
Allow: /funding
Allow: /gallery
Allow: /openings
Allow: /sitemap.xml

# Sitemap location (helps good crawlers index efficiently)
Sitemap: {{ site.url }}{{ site.baseurl }}/sitemap.xml
